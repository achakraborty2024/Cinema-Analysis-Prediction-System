{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMDjWNDEh15UgsRBQQ4eL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achakraborty2024/Cinema-Analysis-Prediction-System/blob/movie_recommendation_module/Capstone_Project_NLP_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Section - Movies recommendation system (Capstone project)\n",
        "\n",
        "### Responsible team member: Rene Ortiz"
      ],
      "metadata": {
        "id": "G1EUvYATENkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling"
      ],
      "metadata": {
        "id": "d-kT3olFGprd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpzq-uoVF5P2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#EDA Profiling library\n",
        "from ydata_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load movie dataset from Google Drive using pandas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Capstone_Project/movies.csv')\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "nYXUhGrFGpoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "3s4S6eGYychj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I created this function after the initial trainings as I realized a json string is not a good strategy, JSON needs to be parse for better results\n",
        "import ast\n",
        "\n",
        "# clean function for genre and keyword fields\n",
        "def extract_names(json_str):\n",
        "    try:\n",
        "        items = ast.literal_eval(json_str)\n",
        "        return \" \".join([item['name'] for item in items if 'name' in item])\n",
        "    except (ValueError, SyntaxError):\n",
        "        return \"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "5U7MKAGNypXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "Xw_gS2fsGpuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Recommendation Models Section :  TF-IDF , BERT and LSTM*"
      ],
      "metadata": {
        "id": "fJmtcXj6q4kP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TERM Frequency-Inverse Document Frequency Recommendation system using the following logic:\n",
        "\n",
        "- Combining features like genres, keywords, and overview text into a single string for each movie.\n",
        "\n",
        "- Converting text into vectors using this techniques: TF-IDF (Term Frequency-Inverse Document Frequency) and CountVectorizer.\n",
        "\n",
        "- Calculating similarity between movies using cosine similarity.\n",
        "\n",
        "- Returning the top-N most similar movies for a given input movie."
      ],
      "metadata": {
        "id": "mrU1-13pSOzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "0ddjptBFQaXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df[['title', 'overview', 'genres', 'keywords', 'popularity', 'release_date']].dropna()\n",
        "\n",
        "# format JSON strings from genre and keyboards\n",
        "df_clean['genres'] = df_clean['genres'].apply(extract_names)\n",
        "df_clean['keywords'] = df_clean['keywords'].apply(extract_names)\n",
        "\n",
        "# Get the from each\n",
        "df_text = df_clean[['title', 'overview', 'genres', 'keywords']]\n",
        "df_text.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "gQTOdVYYQaba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text.head()"
      ],
      "metadata": {
        "id": "cNtID6cRQafE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine text into a single feature\n",
        "def combine_features(row):\n",
        "    return f\"{row['overview']} {row['genres']} {row['keywords']}\"\n",
        "\n",
        "df_text['combined_text'] = df_text.apply(combine_features, axis=1)"
      ],
      "metadata": {
        "id": "psxwlcJkQail"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text.head()"
      ],
      "metadata": {
        "id": "qdDzvP0qQal1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text['combined_text'][0]"
      ],
      "metadata": {
        "id": "b3uCAsZ6Qaok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words and Word Cloud for 5 movies (reference purposes only)\n",
        "def clean_text(text):\n",
        "    tokens = text.lower().split()\n",
        "    return \" \".join([word for word in tokens if word not in stop_words and word.isalpha()])\n",
        "\n",
        "df_text['clean_text'] = df_text['combined_text'].apply(clean_text)\n",
        "\n",
        "# Generate word cloud for first 5 movies\n",
        "for i in range(5):\n",
        "    wc = WordCloud(width=600, height=400, background_color='white').generate(df_text['clean_text'].iloc[i])\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(df['title'].iloc[i])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QMXnDmTvQart"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean text to vectors using TF-IDF"
      ],
      "metadata": {
        "id": "0rsQTKZidrYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = vectorizer.fit_transform(df_text['clean_text'])"
      ],
      "metadata": {
        "id": "NFlm1b7aQavZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.indices"
      ],
      "metadata": {
        "id": "RPM-DR_PeDXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ObL20JHtYwOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean['release_date'] = pd.to_datetime(df_clean['release_date'], errors='coerce').dt.year\n",
        "df_clean['release_year'] = df_clean['release_date'].fillna(0).astype(int)\n",
        "df_clean['popularity'] = pd.to_numeric(df_clean['popularity'], errors='coerce').fillna(0)\n",
        "# metadata: release year and popularity\n",
        "metadata = df_clean[['release_year', 'popularity']].fillna(0)"
      ],
      "metadata": {
        "id": "EIAEBNJWWFJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata.head()"
      ],
      "metadata": {
        "id": "0uwYxwl7uWuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the metadata\n",
        "scaler = MinMaxScaler()\n",
        "normalized_metadata = scaler.fit_transform(metadata[['release_year', 'popularity']])"
      ],
      "metadata": {
        "id": "YTXCXNglWLWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine TF-IDF vectors with metadata (This steps needs a GPU otherwise it takes significant time)\n",
        "tfidf_dense = tfidf_matrix.toarray()"
      ],
      "metadata": {
        "id": "7Yn4LhNRWREF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stack features\n",
        "hybrid_features_tfidf = np.hstack([tfidf_dense, normalized_metadata])"
      ],
      "metadata": {
        "id": "vXyPrQO7WeLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Cosine Similarity\n",
        "#cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "cosine_sim = cosine_similarity(hybrid_features_tfidf)"
      ],
      "metadata": {
        "id": "oTTbLCVyQayb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim"
      ],
      "metadata": {
        "id": "wnym4AaeQa2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute similarity matrix for the hybrid TF-IDF + metadata model\n",
        "similarity_matrix = cosine_similarity(hybrid_features_tfidf)"
      ],
      "metadata": {
        "id": "z4Dk66mbgCeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to call recommendations, sim-scores, genre the TMDB API (queryposters and cast)"
      ],
      "metadata": {
        "id": "5xuNsB56y3aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# API key\n",
        "api_key = \"b400409e22d456acb002b98fa90b2c2d\" # I got this key by registering on TMDB website\n",
        "\n",
        "# get poster URL from TMDb\n",
        "def get_poster_url(movie_title):\n",
        "    try:\n",
        "        url = f\"https://api.themoviedb.org/3/search/movie?api_key={api_key}&query={movie_title}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        if data[\"results\"] and data[\"results\"][0].get(\"poster_path\"):\n",
        "            poster_path = data[\"results\"][0][\"poster_path\"]\n",
        "            return f\"https://image.tmdb.org/t/p/w300{poster_path}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching poster for {movie_title}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "BGcyHa3JhERl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation function with poster display\n",
        "def recommend_movies(title, top_n=5):\n",
        "    idx = df_clean[df_clean['title'].str.lower() == title.lower()].index\n",
        "    if len(idx) == 0:\n",
        "        print(\"Movie not found.\")\n",
        "        return\n",
        "\n",
        "    idx = idx[0]\n",
        "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    recommendations = df_clean[['title', 'genres', 'keywords', 'overview']].iloc[movie_indices].copy()\n",
        "    recommendations['similarity_score'] = [sim[1] for sim in sim_scores]\n",
        "\n",
        "    # Display posters and details\n",
        "    for _, row in recommendations.iterrows():\n",
        "        title = row['title']\n",
        "        poster_url = get_poster_url(title)\n",
        "        print(f\"\\n {title} (Similarity Score: {row['similarity_score']:.3f})\")\n",
        "        print(f\"Genres: {row['genres']}\")\n",
        "        print(f\"Keywords: {row['keywords']}\")\n",
        "        if poster_url:\n",
        "            display(Image(url=poster_url))\n",
        "        else:\n",
        "            print(\"Poster not found.\")\n",
        "\n",
        "    return recommendations.sort_values(by='similarity_score', ascending=False)"
      ],
      "metadata": {
        "id": "0GbZUCno05uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_recommendation(input_title, recommended_df):\n",
        "    input_row = df_clean[df_clean['title'].str.lower() == input_title.lower()].iloc[0]\n",
        "    input_genres = set(input_row['genres'].split(','))\n",
        "    input_keywords = set(input_row['keywords'].split(','))\n",
        "\n",
        "    explanations = []\n",
        "\n",
        "    for _, row in recommended_df.iterrows():\n",
        "        rec_genres = set(row['genres'].split(','))\n",
        "        rec_keywords = set(row['keywords'].split(','))\n",
        "        common_genres = input_genres.intersection(rec_genres)\n",
        "        common_keywords = input_keywords.intersection(rec_keywords)\n",
        "\n",
        "        explanation = {\n",
        "            'title': row['title'],\n",
        "            'similarity_score': row['similarity_score'],\n",
        "            'shared_genres': ', '.join(common_genres),\n",
        "            'shared_keywords': ', '.join(common_keywords)\n",
        "        }\n",
        "        explanations.append(explanation)\n",
        "\n",
        "    return pd.DataFrame(explanations)\n"
      ],
      "metadata": {
        "id": "yTYpP1UPhEJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recs = recommend_movies(\"Superman\", top_n=5)\n",
        "explanations = explain_recommendation(\"Superman\", recs)\n",
        "display(explanations)"
      ],
      "metadata": {
        "id": "FdsK4toKhEOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hvut5XLEhEUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nGDvZt8hEXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Clustering"
      ],
      "metadata": {
        "id": "clGo46JDfKD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "MyI7ZOpdGp3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(df_text['combined_text'])"
      ],
      "metadata": {
        "id": "BSa9RFsOeQq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = 5  # We can try less or more depending how we want to present this on the project\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df_text['cluster'] = kmeans.fit_predict(X)"
      ],
      "metadata": {
        "id": "o5vu21FmeQud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2, random_state=42)\n",
        "reduced = pca.fit_transform(X.toarray())\n",
        "\n",
        "df_text['pca1'] = reduced[:, 0]\n",
        "df_text['pca2'] = reduced[:, 1]"
      ],
      "metadata": {
        "id": "eP3PBd6BeQxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"tab10\", data=df_text, s=60, alpha=0.7\n",
        ")\n",
        "# For refence, I'm adding labels for some sample movies\n",
        "sample_titles = df_text.groupby('cluster').apply(lambda x: x.sample(1, random_state=42))\n",
        "for _, row in sample_titles.iterrows():\n",
        "    plt.text(row['pca1'], row['pca2'], row['title'], fontsize=9)\n",
        "plt.title(\"Movie Clusters Based on Content (TF-IDF + KMeans + PCA)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QPmYj8yoeQ0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Re-use your TF-IDF vectorizer\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "print(\"\\nTop terms per cluster:\")\n",
        "for i in range(num_clusters):\n",
        "    print(f\"\\nCluster {i}:\")\n",
        "    for j in range(10):\n",
        "        print(f\"  {terms[order_centroids[i, j]]}\")"
      ],
      "metadata": {
        "id": "89X1Lav2gVGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=50, n_iter=300, random_state=42)\n",
        "X_embedded = tsne.fit_transform(X.toarray())\n",
        "\n",
        "df_text['tsne1'], df_text['tsne2'] = X_embedded[:,0], X_embedded[:,1]\n",
        "\n",
        "# Plot with t-SNE\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='tsne1', y='tsne2', hue='cluster', data=df_text, palette='tab10', alpha=0.7)\n",
        "plt.title(\"Movie Clusters Based on Content (TF-IDF + KMeans + t-SNE)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-2eIIyGtiCMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Approach - BERT\n",
        "\n",
        "This next section will combine multiple text and numeric features:\n",
        "\n",
        "- Textual features (overview, keywords)\n",
        "\n",
        "- Metadata (genre, release year, cast)\n",
        "\n",
        "- Ratings / popularity scores"
      ],
      "metadata": {
        "id": "3UeIDq4ukajW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "mYgb73AwiCP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "GGvonFzXszeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df[['title', 'overview', 'genres', 'keywords', 'popularity', 'release_date']].dropna()\n",
        "\n",
        "# format JSON strings from genre and keyboards\n",
        "df_clean['genres'] = df_clean['genres'].apply(extract_names)\n",
        "df_clean['keywords'] = df_clean['keywords'].apply(extract_names)\n",
        "\n",
        "# Get the from each\n",
        "df_text = df_clean[['title', 'overview', 'genres', 'keywords']]\n",
        "df_text.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "LXhp_jX50pxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "id": "5ntUhSyS0p7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text fields for BERT input\n",
        "df_clean['combined_text'] = df['overview'] + \" \" + df['genres'] + \" \" + df['keywords']\n",
        "\n",
        "# release date to year\n",
        "#df_clean['release_date'] = pd.to_datetime(df['release_date'], errors='coerce').dt.year.fillna(0).astype(int)\n",
        "df_clean['release_date'] = pd.to_datetime(df_clean['release_date'], errors='coerce').dt.year\n",
        "df_clean['release_year'] = df_clean['release_date'].fillna(0).astype(int)\n",
        "df_clean['popularity'] = pd.to_numeric(df_clean['popularity'], errors='coerce').fillna(0)\n",
        "# metadata: release year and popularity\n",
        "metadata = df_clean[['release_year', 'popularity']].fillna(0)"
      ],
      "metadata": {
        "id": "F4UBKfN6ktjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load BERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n"
      ],
      "metadata": {
        "id": "67KQKNT-ktnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Due to previous errors, I will replace NaN or non-string values with an empty string\n",
        "df_clean['combined_text'] = df_clean['combined_text'].fillna('').astype(str)\n",
        "# encode combined text\n",
        "bert_embeddings = model.encode(df_clean['combined_text'].tolist(), show_progress_bar=True)"
      ],
      "metadata": {
        "id": "-JnASjuSqVdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# movies numerical metadata\n",
        "#metadata = df_clean[['release_date', 'popularity']].fillna(0)\n",
        "\n",
        "# normalize\n",
        "scaler = MinMaxScaler()\n",
        "normalized_metadata = scaler.fit_transform(metadata)\n",
        "\n",
        "# BERT + Metadata\n",
        "hybrid_features = np.hstack([bert_embeddings, normalized_metadata])"
      ],
      "metadata": {
        "id": "BK12QDqektra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computer similirity cosine\n",
        "similarity_matrix = cosine_similarity(hybrid_features)"
      ],
      "metadata": {
        "id": "sR6ahR1piCSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updated recommendation movies w/ similarity scores, genre and keywords\n",
        "def recommend_movies(title, top_n=10):\n",
        "    idx = df_clean[df_clean['title'].str.lower() == title.lower()].index\n",
        "    if len(idx) == 0:\n",
        "        print(\"Movie not found.\")\n",
        "        return\n",
        "\n",
        "    idx = idx[0]\n",
        "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    recommendations = df_clean[['title', 'release_year', 'genres', 'keywords', 'overview']].iloc[movie_indices].copy()\n",
        "    recommendations['similarity_score'] = [sim[1] for sim in sim_scores]\n",
        "\n",
        "    for _, row in recommendations.iterrows():\n",
        "        movie_title = row['title']\n",
        "        release_year = row['release_year']\n",
        "        print(f\"\\n {movie_title} ({release_year}) — Similarity Score: {row['similarity_score']:.3f}\")\n",
        "        print(f\"Genres: {row['genres']}\")\n",
        "        print(f\"Keywords: {row['keywords']}\")\n",
        "        poster_url = get_poster_url(movie_title)\n",
        "        if poster_url:\n",
        "            display(Image(url=poster_url))\n",
        "        else:\n",
        "            print(\"Poster not found.\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    return recommendations.sort_values(by='similarity_score', ascending=False)"
      ],
      "metadata": {
        "id": "8Azr5aJsi5s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# API key\n",
        "api_key = \"b400409e22d456acb002b98fa90b2c2d\" # I got this key by registering on TMDB website\n",
        "\n",
        "# get poster URL from TMDb\n",
        "def get_poster_url(movie_title):\n",
        "    try:\n",
        "        url = f\"https://api.themoviedb.org/3/search/movie?api_key={api_key}&query={movie_title}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        if data[\"results\"] and data[\"results\"][0].get(\"poster_path\"):\n",
        "            poster_path = data[\"results\"][0][\"poster_path\"]\n",
        "            return f\"https://image.tmdb.org/t/p/w300{poster_path}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching poster for {movie_title}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "AQE5FfATefyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_movies(\"Toy Story\", top_n=5)"
      ],
      "metadata": {
        "id": "gAQG5KXooRSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-meamns clustering for Bert recommendation system"
      ],
      "metadata": {
        "id": "zZaqImCuuzsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "yZ3O-k_roRV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(bert_embeddings)"
      ],
      "metadata": {
        "id": "RBdLddJ6oRZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(bert_embeddings)\n",
        "df['pca1'] = pca_result[:, 0]\n",
        "df['pca2'] = pca_result[:, 1]"
      ],
      "metadata": {
        "id": "ncbYVAv4oRcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(data=df, x='pca1', y='pca2', hue='cluster', palette='tab10', alpha=0.6)\n",
        "\n",
        "# Add labels for a few example movies (1 per cluster)\n",
        "sample_titles = df.groupby('cluster').apply(lambda x: x.sample(1, random_state=11)).reset_index(drop=True)\n",
        "for _, row in sample_titles.iterrows():\n",
        "    plt.text(row['pca1'], row['pca2'], row['title'], fontsize=9)\n",
        "\n",
        "plt.title(\"BERT-based Movie Clusters\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "61MXZns4vCQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTop representative movies per BERT-based cluster:\")\n",
        "for i in range(n_clusters):\n",
        "    print(f\"\\nCluster {i}:\")\n",
        "\n",
        "    # get indices of items in this cluster\n",
        "    cluster_indices = df[df['cluster'] == i].index\n",
        "\n",
        "    # get the centroid of the cluster\n",
        "    centroid = kmeans.cluster_centers_[i].reshape(1, -1)\n",
        "\n",
        "    # compute cosine similarity to the centroid\n",
        "    cluster_embeddings = bert_embeddings[cluster_indices]\n",
        "    sims = cosine_similarity(cluster_embeddings, centroid).flatten()\n",
        "\n",
        "    # get top 5 most representative movies\n",
        "    top_indices = cluster_indices[np.argsort(sims)[-5:][::-1]]\n",
        "    for idx in top_indices:\n",
        "        print(f\"  {df.loc[idx, 'title']} - {df.loc[idx, 'genres']}\")\n"
      ],
      "metadata": {
        "id": "sAd7kCyKvCXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised LSTM Model (recommendation system)."
      ],
      "metadata": {
        "id": "jXyXXakptY33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "IMQhRV0WvCao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataframe (df) from google drive running the top cells\n",
        "\n",
        "# clean and combine text features as with TF-IDF and BERT\n",
        "df_clean = df[['title', 'overview', 'genres', 'keywords', 'popularity', 'release_date']].dropna()\n",
        "\n",
        "# combine text fields into one\n",
        "df_clean['combined_text'] = df_clean['title'] + \" \" + df_clean['overview'] + \" \" + df_clean['genres'] + \" \" + df_clean['keywords']\n",
        "\n",
        "# normalize popularity and release date\n",
        "df_clean['release_date'] = pd.to_datetime(df_clean['release_date'], errors='coerce').dt.year.fillna(0).astype(int)\n",
        "scaler = MinMaxScaler()\n",
        "df_clean[['popularity', 'release_date']] = scaler.fit_transform(df_clean[['popularity', 'release_date']])\n",
        "\n"
      ],
      "metadata": {
        "id": "MexbhUDvvCds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of combined_text for reference purposes\n",
        "df_clean['combined_text'][1]"
      ],
      "metadata": {
        "id": "q2JxcT_FvCgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "id": "UgBvGIdIvkam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format JSON strings from genre and keyboards\n",
        "df_clean['genres'] = df_clean['genres'].apply(extract_names)\n",
        "df_clean['keywords'] = df_clean['keywords'].apply(extract_names)"
      ],
      "metadata": {
        "id": "3hsrqP0fvkeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "id": "1BSb3N0hvkil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text preprocessing and tokenization\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df_clean['combined_text'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df_clean['combined_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "# metadata as additional input\n",
        "metadata_features = df_clean[['popularity', 'release_date']].values"
      ],
      "metadata": {
        "id": "6S49iqdpj4Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
        "\n",
        "# inputs for the model\n",
        "text_input = Input(shape=(100,), name=\"text_input\")\n",
        "meta_input = Input(shape=(2,), name=\"meta_input\")\n",
        "\n",
        "# LSTM on text\n",
        "embedding = Embedding(input_dim=10000, output_dim=64, input_length=100)(text_input)\n",
        "lstm_out = LSTM(64)(embedding)\n",
        "\n",
        "# combine LSTM and metadata\n",
        "merged = Concatenate()([lstm_out, meta_input])\n",
        "dense = Dense(64, activation='relu')(merged)\n",
        "output = Dense(32, activation='relu')(dense)  # This becomes the embedding vector for recommendations\n",
        "\n",
        "# define model\n",
        "model = Model(inputs=[text_input, meta_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YPh6oVUBj4Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy output to learn identity (FYI each movie vector is like a label)\n",
        "X_text = padded_sequences\n",
        "X_meta = metadata_features\n",
        "\n",
        "# random targets for training embeddings\n",
        "y = np.random.rand(len(df_clean), 32)\n",
        "\n",
        "# train the model\n",
        "model.fit([X_text, X_meta], y, epochs=100, batch_size=32)"
      ],
      "metadata": {
        "id": "QJ3Hb13dj4aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get learned embeddings for all movies\n",
        "movie_embeddings = model.predict([X_text, X_meta])\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def recommend_lstm(movie_title, top_n=5):\n",
        "    idx = df_clean[df_clean['title'].str.lower() == movie_title.lower()].index\n",
        "    if len(idx) == 0:\n",
        "        print(\"Movie not found.\")\n",
        "        return\n",
        "\n",
        "    idx = idx[0]\n",
        "    query_embedding = movie_embeddings[idx]\n",
        "    sim_scores = cosine_similarity([query_embedding], movie_embeddings)[0]\n",
        "    top_indices = np.argsort(sim_scores)[::-1][1:top_n+1]\n",
        "\n",
        "    recommendations = df_clean.iloc[top_indices][['title', 'genres', 'keywords', 'overview']].copy()\n",
        "    recommendations['similarity_score'] = sim_scores[top_indices]\n",
        "\n",
        "    return recommendations.sort_values(by='similarity_score', ascending=False)"
      ],
      "metadata": {
        "id": "Hjn-xTQKj4dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_lstm(\"Superman\")"
      ],
      "metadata": {
        "id": "FPevwDaAj4gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLOVE + LSTM Model w/ additional numeric features (Supervised training)\n",
        "\n",
        "## 1st load the df from the top df code / google drive"
      ],
      "metadata": {
        "id": "lSrqyAj216Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "x4SuDu1Nj4jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine text fields\n",
        "df['combined_text'] = df['overview'].fillna('') + \" \" + \\\n",
        "                      df['genres'].fillna('') + \" \" + \\\n",
        "                      df['keywords'].fillna('')"
      ],
      "metadata": {
        "id": "rl5ia29oj4mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text.lower()\n",
        "\n",
        "#apply to the combined text\n",
        "df['combined_text'] = df['combined_text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "ksh_0np82n1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  text tokenize\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['combined_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['combined_text'])\n",
        "X_text = pad_sequences(sequences, maxlen=300)"
      ],
      "metadata": {
        "id": "s0bfnd2E2n4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing all nmerical features\n",
        "df['release_year'] = pd.to_datetime(df['release_date'], errors='coerce').dt.year.fillna(0).astype(int)\n",
        "df[['popularity', 'vote_average', 'vote_count', 'runtime']] = df[['popularity', 'vote_average', 'vote_count', 'runtime']].fillna(0)\n",
        "numerical = df[['release_year', 'popularity', 'vote_average', 'vote_count', 'runtime']]\n",
        "scaler = MinMaxScaler()\n",
        "X_num = scaler.fit_transform(numerical)"
      ],
      "metadata": {
        "id": "GZDBihQ72n7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# language encoding\n",
        "df['original_language'] = df['original_language'].fillna('unknown')\n",
        "le = LabelEncoder()\n",
        "X_lang = le.fit_transform(df['original_language']).reshape(-1, 1)\n",
        "\n",
        "# numerical + language\n",
        "X_meta = np.hstack((X_num, X_lang))\n",
        "\n",
        "# input for LSTM\n",
        "X_final = [X_text, X_meta]"
      ],
      "metadata": {
        "id": "Kezu8qqd2n9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_final"
      ],
      "metadata": {
        "id": "E8NVSFKZ2oAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# similarity scores based on vote average\n",
        "vote_scores = df['vote_average'].values.reshape(-1, 1)\n",
        "y_similarity = cosine_similarity(vote_scores)"
      ],
      "metadata": {
        "id": "KkGUedO62oD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "c1a61603"
      },
      "source": [
        "# download GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Glove embedding\n",
        "embedding_index = {}\n",
        "with open('/content/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "embedding_dim = 100\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((10000, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= 10000:\n",
        "        continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "jLMa2KYCj4pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Dropout\n",
        "\n",
        "# text input\n",
        "text_input = Input(shape=(300,))\n",
        "embed = Embedding(input_dim=10000, output_dim=100, weights=[embedding_matrix], input_length=300, trainable=False)(text_input)\n",
        "lstm_out = LSTM(64)(embed)\n",
        "\n",
        "# metadata input\n",
        "meta_input = Input(shape=(X_meta.shape[1],))\n",
        "meta_dense = Dense(32, activation='relu')(meta_input)\n",
        "\n",
        "# combine\n",
        "combined = Concatenate()([lstm_out, meta_dense])\n",
        "combined = Dropout(0.3)(combined)\n",
        "output = Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "model = Model(inputs=[text_input, meta_input], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UTjNtFqwj4sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample dummy labels (1 if vote_average diff < 0.5 else 0)\n",
        "labels = np.where(abs(df['vote_average'].values - df['vote_average'].values.mean()) < 0.5, 1, 0)\n",
        "\n",
        "model.fit(X_final, labels, epochs=50, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "De4ZeGOa4e0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# xtract features from the penultimate layer\n",
        "feature_extractor = Model(inputs=model.input, outputs=model.get_layer(index=-2).output)\n",
        "\n",
        "# feature embeddings for all movies\n",
        "movie_embeddings = feature_extractor.predict(X_final, batch_size=32)"
      ],
      "metadata": {
        "id": "ZxY7Q39k4e39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute pairwise cosine similarity between all movies\n",
        "similarity_matrix = cosine_similarity(movie_embeddings)"
      ],
      "metadata": {
        "id": "sqOwfTMc4e7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_movies_lstm(title, top_n=5):\n",
        "    # find movie index\n",
        "    idx = df[df['title'].str.lower() == title.lower()].index\n",
        "    if len(idx) == 0:\n",
        "        print(\"Movie not found.\")\n",
        "        return\n",
        "    idx = idx[0]\n",
        "\n",
        "    # Get similarity scores for the movie\n",
        "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Skip the movie itself (first match) and select top-N\n",
        "    sim_scores = sim_scores[1:top_n + 1]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    print(f\"\\nTop {top_n} similar movies to: {df.iloc[idx]['title']}\")\n",
        "    for i in movie_indices:\n",
        "        title = df.iloc[i]['title']\n",
        "        score = sim_scores[movie_indices.index(i)][1]\n",
        "        print(f\"{title} — Similarity Score: {score:.3f}\")"
      ],
      "metadata": {
        "id": "kwVsrFAX4e-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_movies_lstm(\"Transformers\", top_n=5)"
      ],
      "metadata": {
        "id": "-fLxOEYn4fB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# cluster into 5 groups\n",
        "num_clusters = 5\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(movie_embeddings)"
      ],
      "metadata": {
        "id": "8MPVq3EjMiBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 movies per cluster for inspection\n",
        "for i in range(num_clusters):\n",
        "    print(f\"\\nCluster {i}:\")\n",
        "    sample_movies = df[df['cluster'] == i].sample(5, random_state=42)\n",
        "    for title in sample_movies['title']:\n",
        "        print(f\"  - {title}\")"
      ],
      "metadata": {
        "id": "ZcttFbl7MiFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nTop representative movies per LSTM-based cluster:\")\n",
        "for i in range(num_clusters):\n",
        "    print(f\"\\nCluster {i}:\")\n",
        "\n",
        "    # Get indices of items in this cluster\n",
        "    cluster_indices = df[df['cluster'] == i].index\n",
        "\n",
        "    # Get the centroid of the cluster\n",
        "    centroid = kmeans.cluster_centers_[i].reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarity to the centroid\n",
        "    cluster_embeddings = movie_embeddings[cluster_indices]\n",
        "    sims = cosine_similarity(cluster_embeddings, centroid).flatten()\n",
        "\n",
        "    # Get top 5 most representative movies\n",
        "    top_indices = cluster_indices[np.argsort(sims)[-5:][::-1]]\n",
        "    for idx in top_indices:\n",
        "        print(f\"  {df.loc[idx, 'title']} - {df.loc[idx, 'genres']}\")"
      ],
      "metadata": {
        "id": "S713gt6kMiL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "# Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(movie_embeddings)\n",
        "\n",
        "# Add PCA components to the DataFrame\n",
        "df['pca1'] = reduced_embeddings[:, 0]\n",
        "df['pca2'] = reduced_embeddings[:, 1]\n",
        "df['cluster'] = kmeans.labels_\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(12, 8))\n",
        "palette = sns.color_palette(\"hsv\", len(df['cluster'].unique()))\n",
        "sns.scatterplot(data=df, x='pca1', y='pca2', hue='cluster', palette=palette, alpha=0.7)\n",
        "\n",
        "# Optional: Add movie titles for a few representative samples per cluster\n",
        "sample_titles = df.groupby('cluster').apply(lambda x: x.sample(1, random_state=42))\n",
        "for _, row in sample_titles.iterrows():\n",
        "    plt.text(row['pca1'], row['pca2'], row['title'], fontsize=8)\n",
        "\n",
        "plt.title(\"LSTM-based Movie Clusters (PCA Projection)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1XwfxDFzMiPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genre Classification section BERT vs LSTM w/ GLOVE embeddings"
      ],
      "metadata": {
        "id": "L0TZOxPBlWtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BMXL_DgYGkxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TuBUqoARGk08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GdcFDie-Gk4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vFS2r22nGk7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSw39j6tGk-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}